# IEEE_Hands_On_RL_using_Python
In this workshop, we explore the basics of reinforcement learning using OpenAI gym, and google Colab. The file `IEEE_Hands_On_RL_using_Python.ipynb` contains all the code related to RL. The file can be opened directly in colab. Feel free to clone, fork, and improve upon it. The file is fully FREE to use with or without modifications. Comercial use is fully okay too! I believe learning should be free.

üìåüìå If this workshop helped you, and you want the follow up videos for custom environments üëâ https://www.youtube.com/c/SadiaKhaf92 (Like, share, ask your questions in comments, subscribe!)

Buy me a ‚òïÔ∏è : https://paypal.me/SadiaKhaf

My YouTube channel: https://www.youtube.com/c/SadiaKhaf92
My Website: https://academiccoaching.ml/
My Blog: https://sadiakhaf.blogspot.com/ 
LinkedIn: https://pk.linkedin.com/in/sadiakhaf 
Twitter: https://twitter.com/SadiaKhaf


## We covered the following:

- [x]	Introduction to Colab, creating your first notebook, settings, GPU usage, themes
- [x]	Mounting Google Drive to Colab, uploading and accessing files from drive
- [x]	Importing different libraries in Colab, like Numpy, Tensorflow, PyTorch, Pandas, installing missing libraries such as OpenAI gym
- [x]	Handling images in Colab using matplotlib, and handling data using pickle
- [x]	Introduction to reinforcement learning, difference between supervised, unsupervised, and reinforcement learning (RL), agent and environment description
- [x]	Introduction to OpenAI gym for RL, benefits of standard gym environments, creating your own RL environments
- [x]	Standard functions of agent and environment classes in OpenAI gym environments and how to create/modify your own functions
- [x]	Introduction to model free RL such as Q learning, notion of state space, action space, and Q-tables
- [x]	The concept of spaces in RL and relating them to OpenAI gym, creating custom state and action space
- [ ].	Creating a custom wireless transmitter and receiver environment
- [ ].	Creating a Q-learning code and applying it to the environment we just created
- [ ].	Using, re-using, and re-re-using your environments, agents, policies, and other classes in all your future RL codes

